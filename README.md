# ğŸ¤– AI Documentation Generator

<div align="center">

![Demo](UI%20Example%201.png)

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-ee4c2c.svg)](https://pytorch.org/)
[![Flask](https://img.shields.io/badge/Flask-2.0+-000000.svg)](https://flask.palletsprojects.com/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

**Automatically generate documentation for Python functions using Deep Learning**

[Features](#-features) â€¢ [Demo](#-demo) â€¢ [Installation](#-installation) â€¢ [Usage](#-usage) â€¢ [Training](#-training)

</div>

---

## âœ¨ Features

- ğŸ§  **Deep Learning Model**: 3-layer Bidirectional LSTM with **15,890,283 parameters**
- ğŸ¨ **Beautiful Web UI**: Modern, responsive Flask-based interface
- âš¡ **Real-time Generation**: Instant documentation generation (< 1 second)
- ğŸ“Š **High Accuracy**: 0.0008 validation loss after 15 epochs
- ğŸ¯ **Easy to Use**: Simple one-click examples and intuitive interface
- ğŸ’» **CPU Compatible**: Works on any PC, no GPU required

---

## ğŸ¬ Demo

### ğŸŒ Web Interface

<div align="center">

![UI Example 1](UI%20Example%201.png)
*Main interface with code input and documentation output*

![UI Example 2](UI%20Example%202.png)
*Real-time generation with multiple examples*

![UI Example 3](UI%20Example%203.png)
*Clean, professional UI design*

</div>

### ğŸ“Š Training Process & Results

<div align="center">

![Training Process](Training%20Process.png)
*Model training with real-time metrics showing 15 epochs*

![Training Metrics](Training%20Metrics.png)
*Training progress with loss curves and validation metrics*

</div>

### ğŸ¯ Example Outputs

<div align="center">

![Examples](Examples.png)
*Testing interface with pre-defined examples*

![Results](Results.png)
*Generated documentation results showing model accuracy*

</div>

### ğŸ“‹ Sample Generations

| Input Function | Generated Documentation |
|---------------|------------------------|
| `def add(a, b): return a + b` | âœ… Add two numbers and return their sum |
| `def is_even(n): return n % 2 == 0` | âœ… Check if a number is even |
| `def reverse(s): return s[::-1]` | âœ… Reverse a string |
| `def find_max(a, b): return a if a > b else b` | âœ… Find maximum of two numbers |
| `def multiply(x, y): return x * y` | âœ… Multiply two numbers and return product |

---

## ğŸš€ Installation

### Prerequisites
- Python 3.8 or higher
- pip package manager

### Quick Start

```bash
# 1. Clone the repository
git clone https://github.com/fsmalik110/AI-DOC-Gen.git
cd AI-DOC-Gen

# 2. Install dependencies
pip install -r requirements.txt

# 3. Run the web interface
python web_ui.py
Open your browser: http://localhost:5000

---

## ğŸ’» Usage
Option 1: Web Interface (Recommended)

python web_ui.py

Then navigate to http://localhost:5000 and:

-Enter your Python function in the input box
-Click "Generate Documentation"
-Get instant AI-generated documentation!

Option 2: Command Line Testing

python test_model_enhanced.py

Interactive mode with pre-loaded examples.

Option 3: Python API

import torch
import pickle

# Load model
checkpoint = torch.load('models/best_model.pth', map_location='cpu')

# Your code here
code = "def add(a, b): return a + b"

# Generate documentation
# (Full implementation in test_model.py)

---

## ğŸ“‚ Project Structure
AI-DOC-Gen/
â”‚
â”œâ”€â”€ ğŸ“ data/                       # Dataset files
â”‚   â”œâ”€â”€ train_data.pkl            # Training dataset (10,000 samples)
â”‚   â””â”€â”€ val_data.pkl              # Validation dataset (1,000 samples)
â”‚
â”œâ”€â”€ ğŸ“ models/                     # Trained model files
â”‚   â”œâ”€â”€ best_model.pth            # Best model checkpoint (186 KB)
â”‚   â””â”€â”€ training_history.pkl      # Training metrics history
â”‚
â”œâ”€â”€ ğŸ“ outputs/                    # Generated outputs
â”‚
â”œâ”€â”€ ğŸ“¸ Screenshots/
â”‚   â”œâ”€â”€ UI Example 1.png
â”‚   â”œâ”€â”€ UI Example 2.png
â”‚   â”œâ”€â”€ UI Example 3.png
â”‚   â”œâ”€â”€ Training Process.png
â”‚   â”œâ”€â”€ Training Metrics.png
â”‚   â”œâ”€â”€ Results.png
â”‚   â”œâ”€â”€ Examples.png
â”‚   â””â”€â”€ Project Structure.png
â”‚
â”œâ”€â”€ ğŸ“„ main.py                     # Simple training script
â”œâ”€â”€ ğŸ“„ train_improved_CORRECT.py   # Advanced training pipeline
â”œâ”€â”€ ğŸ“„ test_model.py               # Basic model testing
â”œâ”€â”€ ğŸ“„ test_model_enhanced.py      # Interactive testing interface
â”œâ”€â”€ ğŸ“„ web_ui.py                   # Flask web application
â”œâ”€â”€ ğŸ“„ visualize_training.py       # Training visualization
â”œâ”€â”€ ğŸ“„ create_sample_dataset.py    # Dataset creation
â”œâ”€â”€ ğŸ“„ download_dataset.py         # CodeSearchNet downloader
â”œâ”€â”€ ğŸ“„ requirements.txt            # Python dependencies
â”œâ”€â”€ ğŸ“„ README.md                   # This file
â””â”€â”€ ğŸ“„ .gitignore                  # Git ignore rules

---

## ğŸ—ï¸ Model Architecture
## Network Design

Input (Python Code)
    â†“
Tokenization & Embedding (256 dim)
    â†“
3Ã— Bidirectional LSTM (512 hidden units each)
    â†“
Dropout Layer (0.3)
    â†“
Fully Connected Layer
    â†“
Softmax Output
    â†“
Documentation Text

---

### Technical Specifications

| Component | Configuration |
|-----------|--------------|
| **Architecture** | 3-layer Bidirectional LSTM |
| **Embedding Dimension** | 256 |
| **Hidden Units** | 512 per layer |
| **Total Parameters** | 15,890,283 |
| **Dropout Rate** | 0.3 |
| **Vocabulary Size** | 10,000 tokens |
| **Max Sequence Length** | 100 tokens |
| **Optimizer** | Adam (lr=0.001) |
| **Loss Function** | Cross Entropy Loss |
| **Batch Size** | 32 |

---
## ğŸ“ Training Process

### Dataset

**Source:** CodeSearchNet (Python subset)
- **Training Samples:** 10,000 Python functions with documentation
- **Validation Samples:** 1,000 Python functions
- **Vocabulary:** 10,000 most common tokens

### Training Configuration

```python
Epochs: 15
Batch Size: 32
Learning Rate: 0.001 (with ReduceLROnPlateau)
Early Stopping: Patience = 5 epochs
Optimizer: Adam
Loss Function: Cross Entropy
Device: CPU/CUDA (auto-detect)

---
### Training Results

**Final Metrics:**
- âœ… **Training Loss:** 0.0001
- âœ… **Validation Loss:** 0.0008
- âœ… **Learning Rate:** 1e-06 (after scheduling)
- âœ… **Epochs Completed:** 15/15
- âœ… **Model Size:** 186 KB

![Training Curves](Training%20Metrics.png)
*Training and validation loss curves over 15 epochs*

---

## ğŸ”¬ Train Your Own Model

### Step 1: Prepare Dataset

 Option A: Download full CodeSearchNet dataset (large, ~2GB)
python download_dataset.py

 Option B: Create sample dataset (quick, for testing)
python create_sample_dataset.py

### Step 2: Train Model

### Train with full pipeline
python train_improved_CORRECT.py

## Training will:

- âœ… Load and process dataset
- âœ… Build vocabulary from code/docs
- âœ… Create data loaders
- âœ… Train BiLSTM model
- âœ… Save best model checkpoint
- âœ… Generate training history

### Expected Time:

CPU: ~30-60 minutes
GPU: ~10-20 minutes

## Step 3: Visualize Results

python visualize_training.py

